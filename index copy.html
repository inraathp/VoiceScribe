<!DOCTYPE html>
<html lang="en" data-bs-theme="dark">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voice Recording, Transcription, and Summarization</title>
  
  <!-- Bootstrap CSS (Replit theme) -->
  <link rel="stylesheet" href="https://cdn.replit.com/agent/bootstrap-agent-dark-theme.min.css">
  
  <!-- Custom styles -->
  <style>
    /* Custom styles for the voice recording and transcription app */
    html {
      overflow-y: scroll; /* Always show vertical scrollbar */
      scroll-behavior: smooth; /* Smooth scrolling for anchor links */
    }
    
    body {
      padding-top: 20px;
      padding-bottom: 40px;
      min-height: 100vh; /* Full viewport height */
      overflow-x: hidden; /* Prevent horizontal scrolling */
    }
    
    .container {
      max-width: 960px;
      overflow-y: auto; /* Enable vertical scrolling for containers */
    }
    
    .app-header {
      margin-bottom: 1.5rem;
      border-bottom: 1px solid var(--bs-gray-700);
      padding-bottom: 1rem;
    }
    
    .app-title {
      font-size: 2rem;
    }
    
    .app-subtitle {
      color: var(--bs-gray-500);
    }
    
    .control-panel {
      margin-bottom: 2rem;
      padding: 1.5rem;
      border-radius: 0.25rem;
      background-color: var(--bs-dark);
    }
    
    .transcription-box, .summary-box {
      height: 300px;
      overflow-y: auto;
      padding: 1rem;
      border-radius: 0.25rem;
      background-color: var(--bs-gray-900);
      border: 1px solid var(--bs-gray-700);
      white-space: pre-wrap;
      font-family: 'Courier New', monospace;
      /* Improved scrollbar styling */
      scrollbar-width: thin;
      scrollbar-color: var(--bs-gray-600) var(--bs-gray-800);
    }
    
    /* Custom scrollbar for Webkit browsers */
    .transcription-box::-webkit-scrollbar,
    .summary-box::-webkit-scrollbar {
      width: 8px;
    }
    
    .transcription-box::-webkit-scrollbar-track,
    .summary-box::-webkit-scrollbar-track {
      background: var(--bs-gray-800);
      border-radius: 4px;
    }
    
    .transcription-box::-webkit-scrollbar-thumb,
    .summary-box::-webkit-scrollbar-thumb {
      background-color: var(--bs-gray-600);
      border-radius: 4px;
    }
    
    .status-message {
      margin-top: 1rem;
      margin-bottom: 1rem;
    }
    
    .button-group {
      margin-top: 1rem;
    }
    
    .recording-indicator {
      display: inline-block;
      width: 10px;
      height: 10px;
      border-radius: 50%;
      margin-right: 5px;
      background-color: var(--bs-danger);
      animation: pulse 1.5s infinite;
    }
    
    .section-title {
      margin-bottom: 1rem;
      padding-bottom: 0.5rem;
      border-bottom: 1px solid var(--bs-gray-700);
    }
    
    .hidden {
      display: none;
    }
    
    /* Pulsing animation for recording indicator */
    @keyframes pulse {
      0% {
        opacity: 0.6;
      }
      50% {
        opacity: 1;
      }
      100% {
        opacity: 0.6;
      }
    }
    
    /* Audio source selector */
    .audio-source-selector {
      margin-bottom: 1rem;
    }
    
    /* Responsive adjustments */
    @media (max-width: 768px) {
      .control-panel {
        padding: 1rem;
      }
      
      .transcription-box, .summary-box {
        height: 200px;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <!-- App Header -->
    <header class="app-header text-center">
      <h1 class="app-title">Voice Recording & Transcription</h1>
      <p class="app-subtitle">Record, transcribe, and summarize your conversations</p>
    </header>
    
    <!-- Status Messages -->
    <div id="status" class="alert alert-info status-message" role="alert">
      Enter a filename and press "Start Recording" to begin.
    </div>
    
    <!-- Control Panel -->
    <div class="control-panel">
      <div class="row">
        <div class="col-md-8">
          <div class="mb-3">
            <label for="filename" class="form-label">Filename</label>
            <input type="text" class="form-control" id="filename" placeholder="Enter filename">
          </div>
          
          <!-- Audio Source Selection -->
          <div class="mb-3 audio-source-selector">
            <label class="form-label">Recording Source</label>
            <div class="form-check">
              <input class="form-check-input" type="radio" name="audioSource" id="microphoneOnly" value="microphone" checked>
              <label class="form-check-label" for="microphoneOnly">
                Microphone Only <span class="badge bg-success">Recommended</span>
              </label>
              <small class="form-text text-muted d-block">Works in all browsers and environments.</small>
            </div>
            <div class="form-check mt-2">
              <input class="form-check-input" type="radio" name="audioSource" id="systemAudio" value="system">
              <label class="form-check-label" for="systemAudio">
                System Audio (Speaker Sound) <span class="badge bg-warning">Experimental</span>
              </label>
              <small class="form-text text-muted d-block">Records audio playing through your speakers. May not work in all browsers or environments. Will request screen sharing permission (you can share just the audio).</small>
            </div>
            <div class="form-check mt-2">
              <input class="form-check-input" type="radio" name="audioSource" id="bothSources" value="both">
              <label class="form-check-label" for="bothSources">
                Both Microphone and System Audio <span class="badge bg-warning">Experimental</span>
              </label>
              <small class="form-text text-muted d-block">Records both your microphone and system audio simultaneously. Will request screen sharing permission (you can share just the audio). May fall back to one of the other options if not supported.</small>
            </div>
          </div>
        </div>
      </div>
      
      <!-- API Key Input -->
      <div class="mb-3">
        <label for="apiKey" class="form-label">OpenAI API Key (required for transcription and summarization)</label>
        <input type="password" class="form-control" id="apiKey" placeholder="Enter your OpenAI API key">
        <div class="form-text">Your API key is only stored in your browser and is never sent to our servers.</div>
      </div>
      
      <div class="button-group">
        <button id="startRecording" class="btn btn-primary">
          <i class="bi bi-mic-fill"></i> Start Recording
        </button>
        <button id="stopRecording" class="btn btn-danger" disabled>
          <i class="bi bi-stop-fill"></i> Stop Recording
        </button>
        <button id="abort" class="btn btn-secondary">
          <i class="bi bi-x-circle"></i> Abort
        </button>
      </div>
    </div>
    
    <!-- Audio Preview -->
    <div id="audioPreviewSection" class="mb-4" style="display: none;">
      <h3 class="section-title">Audio Preview</h3>
      <audio id="audioPreview" controls style="width: 100%;"></audio>
      <div class="mt-2">
        <button id="transcribeBtn" class="btn btn-primary">
          <i class="bi bi-file-earmark-text"></i> Transcribe Audio
        </button>
      </div>
    </div>
    
    <!-- Transcription Section -->
    <div id="transcriptionSection" class="mb-4" style="display: none;">
      <h3 class="section-title">Transcription</h3>
      <div id="transcription" class="transcription-box"></div>
      <div class="mt-2">
        <button id="downloadTranscript" class="btn btn-outline-info">
          <i class="bi bi-download"></i> Download Transcript
        </button>
      </div>
      
      <!-- Summarization Controls -->
      <div class="mt-4">
        <h4>Generate Summary</h4>
        <div class="mb-3">
          <label for="summaryPrompt" class="form-label">Summary Prompt (optional)</label>
          <input type="text" class="form-control" id="summaryPrompt" 
                 placeholder="Summarize this conversation" 
                 value="Summarize this conversation">
        </div>
        <button id="summarize" class="btn btn-success">
          <i class="bi bi-file-earmark-text"></i> Generate Summary
        </button>
      </div>
    </div>
    
    <!-- Summary Section -->
    <div id="summarySection" class="mb-4" style="display: none;">
      <h3 class="section-title">Summary</h3>
      <div id="summary" class="summary-box"></div>
      <div class="mt-2">
        <button id="downloadSummary" class="btn btn-outline-info">
          <i class="bi bi-download"></i> Download Summary
        </button>
      </div>
    </div>
    
    <!-- Footer -->
    <footer class="text-center text-muted mt-5">
      <p>This application uses OpenAI Whisper for transcription and AI-powered speaker diarization.</p>
    </footer>
  </div>
  
  <!-- JavaScript -->
  <script>
    document.addEventListener('DOMContentLoaded', () => {
      // UI Elements
      const filenameInput = document.getElementById('filename');
      const apiKeyInput = document.getElementById('apiKey');
      const startRecordingBtn = document.getElementById('startRecording');
      const stopRecordingBtn = document.getElementById('stopRecording');
      const abortBtn = document.getElementById('abort');
      const statusMessage = document.getElementById('status');
      const audioPreviewSection = document.getElementById('audioPreviewSection');
      const audioPreview = document.getElementById('audioPreview');
      const transcribeBtn = document.getElementById('transcribeBtn');
      const transcriptionSection = document.getElementById('transcriptionSection');
      const transcriptionDiv = document.getElementById('transcription');
      const downloadTranscriptBtn = document.getElementById('downloadTranscript');
      const summaryPrompt = document.getElementById('summaryPrompt');
      const summarizeBtn = document.getElementById('summarize');
      const summarySection = document.getElementById('summarySection');
      const summaryDiv = document.getElementById('summary');
      const downloadSummaryBtn = document.getElementById('downloadSummary');
      
      // Audio source radio buttons
      const microphoneOnlyRadio = document.getElementById('microphoneOnly');
      const systemAudioRadio = document.getElementById('systemAudio');
      const bothSourcesRadio = document.getElementById('bothSources');
      
      // App state
      let appState = 'idle'; // idle, recording, transcribing, transcribed, summarizing
      let mediaRecorder = null;
      let audioChunks = [];
      let audioBlob = null;
      let stream = null;
      
      // Check for stored API key
      const storedApiKey = localStorage.getItem('openai_api_key');
      if (storedApiKey) {
        apiKeyInput.value = storedApiKey;
      }
      
      // Event listeners
      startRecordingBtn.addEventListener('click', startRecording);
      stopRecordingBtn.addEventListener('click', stopRecording);
      abortBtn.addEventListener('click', abortProcess);
      transcribeBtn.addEventListener('click', transcribeAudio);
      summarizeBtn.addEventListener('click', summarizeTranscription);
      downloadTranscriptBtn.addEventListener('click', downloadTranscript);
      downloadSummaryBtn.addEventListener('click', downloadSummary);
      
      // Save API key when it changes
      apiKeyInput.addEventListener('change', () => {
        localStorage.setItem('openai_api_key', apiKeyInput.value);
      });
      
      // Functions
      async function startRecording() {
        const filename = filenameInput.value.trim();
        if (!filename) {
          showError('Please enter a filename first');
          return;
        }
        
        try {
          // Reset UI
          transcriptionDiv.textContent = '';
          summaryDiv.textContent = '';
          audioPreviewSection.style.display = 'none';
          transcriptionSection.style.display = 'none';
          summarySection.style.display = 'none';
          
          // Get the selected audio source
          const audioSource = getSelectedAudioSource();
          
          // Set up the appropriate stream based on audio source selection
          stream = await getAudioStream(audioSource);
          
          if (!stream) {
            showError('Failed to access the selected audio source. Please check permissions.');
            return;
          }
          
          // Create media recorder
          mediaRecorder = new MediaRecorder(stream);
          audioChunks = [];
          
          mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
              audioChunks.push(event.data);
            }
          };
          
          mediaRecorder.onstop = () => {
            // Create audio blob and URL
            audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
            const audioUrl = URL.createObjectURL(audioBlob);
            
            // Update audio player
            audioPreview.src = audioUrl;
            audioPreviewSection.style.display = 'block';
            
            // Update state
            appState = 'recorded';
            updateUIState();
            showStatus('Recording complete. You can now transcribe the audio.');
          };
          
          // Start recording
          mediaRecorder.start();
          
          // Update state and UI
          appState = 'recording';
          updateUIState();
          showStatus('Recording... Speak now.');
          
        } catch (error) {
          console.error('Error starting recording:', error);
          showError(`Failed to start recording: ${error.message}`);
        }
      }
      
      function getSelectedAudioSource() {
        if (microphoneOnlyRadio.checked) return 'microphone';
        if (systemAudioRadio.checked) return 'system';
        if (bothSourcesRadio.checked) return 'both';
        return 'microphone'; // Default
      }
      
      async function getAudioStream(source) {
        try {
          switch (source) {
            case 'microphone':
              return await navigator.mediaDevices.getUserMedia({ 
                audio: {
                  echoCancellation: true,
                  noiseSuppression: true,
                  autoGainControl: true
                } 
              });
              
            case 'system':
              try {
                // Check if getDisplayMedia is supported
                if (!navigator.mediaDevices.getDisplayMedia) {
                  showStatus('System audio recording is not supported in this browser.');
                  return await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                      echoCancellation: false,
                      noiseSuppression: false,
                      autoGainControl: false
                    } 
                  });
                }
                
                const stream = await navigator.mediaDevices.getDisplayMedia({
                  video: { 
                    width: 1, 
                    height: 1, 
                    frameRate: 1 
                  },
                  audio: true
                });
                
                // Check if we got audio tracks
                if (stream.getAudioTracks().length === 0) {
                  throw new Error("No audio track available in the captured display media");
                }
                
                return stream;
              } catch (err) {
                console.error("Error capturing system audio:", err);
                showStatus('System audio recording failed. Trying microphone only...');
                // Fall back to microphone only
                return await navigator.mediaDevices.getUserMedia({ 
                  audio: true 
                });
              }
              
              case 'both':
                try {
    // Check if getDisplayMedia is supported
                  if (!navigator.mediaDevices.getDisplayMedia) {
      showStatus('Combined audio recording is not supported in this browser.');
      return await navigator.mediaDevices.getUserMedia({ audio: true });
    }
    
    // For both sources, first get system audio
    const displayStream = await navigator.mediaDevices.getDisplayMedia({
      video: { width: 1, height: 1, frameRate: 1 },
      audio: true
    });
    
    // If we didn't get audio tracks from display, throw an error
    if (displayStream.getAudioTracks().length === 0) {
      throw new Error("No audio track available in the captured display media");
    }
    
    // Then try to get microphone access
    try {
      const micStream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: false,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      // Use Web Audio API to combine streams
      const audioContext = new AudioContext();
      
      // Create sources for each stream
      const desktopSource = audioContext.createMediaStreamSource(displayStream);
      const micSource = audioContext.createMediaStreamSource(micStream);
      
      // Create a destination to combine them
      const destination = audioContext.createMediaStreamDestination();
      
      // Optional: Create gain nodes to control volume levels
      const desktopGain = audioContext.createGain();
      const micGain = audioContext.createGain();
      
      // Set gain values (adjust as needed)
      desktopGain.gain.value = 0.7;
      micGain.gain.value = 0.7;
      
      // Connect the sources to the destination through the gain nodes
      desktopSource.connect(desktopGain).connect(destination);
      micSource.connect(micGain).connect(destination);
      
      // Return the combined audio tracks
      return new MediaStream(destination.stream.getAudioTracks());
      
    } catch (micError) {
      console.warn('Could not access microphone, falling back to system audio only:', micError);
      showStatus('Could not access microphone, recording system audio only.');
      return displayStream;
    }
  } catch (err) {
    console.error("Error capturing combined audio:", err);
    showStatus('Combined audio recording failed. Using microphone only...');
    // Fall back to microphone only
    return await navigator.mediaDevices.getUserMedia({ audio: true });
  }

              
            default:
              return await navigator.mediaDevices.getUserMedia({ audio: true });
          }
        } catch (error) {
          console.error('Error accessing media devices:', error);
          throw new Error("Not supported");
        }
      }
      
      function stopRecording() {
        if (appState !== 'recording' || !mediaRecorder) return;
        
        showStatus('Stopping recording...');
        mediaRecorder.stop();
        
        // Stop all tracks on the stream
        if (stream) {
          stream.getTracks().forEach(track => track.stop());
        }
      }
      
      async function transcribeAudio() {
        if (!audioBlob) {
          showError('No recording available to transcribe');
          return;
        }
        
        const apiKey = apiKeyInput.value.trim();
        if (!apiKey) {
          showError('Please enter your OpenAI API key');
          return;
        }
        
        try {
          // Update state and UI
          appState = 'transcribing';
          updateUIState();
          showStatus('Transcribing audio... This may take a few minutes.');
          
          // Convert audio to format suitable for OpenAI API (MP3 or WAV)
          const formData = new FormData();
          formData.append('file', audioBlob, 'recording.wav');
          formData.append('model', 'whisper-1');
          formData.append('response_format', 'verbose_json');
          
          // Call OpenAI API directly from the browser
          const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${apiKey}`
            },
            body: formData
          });
          
          if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.error?.message || 'Failed to transcribe audio');
          }
          
          const transcriptionData = await response.json();
          
          // Now perform speaker diarization using OpenAI's chat completions
          appState = 'diarizing';
          updateUIState();
          showStatus('Identifying speakers in the transcript...');
          
          const diarizationResponse = await fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
              model: "gpt-4o", // Use latest model
              messages: [
                {
                  role: "system",
                  content: "You are an expert in conversation analysis. Your task is to identify different speakers in a transcript and format it with speaker labels. Use 'Speaker 1:', 'Speaker 2:', etc. If you can't confidently determine speaker changes, make your best estimate based on conversation flow, questions and answers, or topic changes."
                },
                {
                  role: "user",
                  content: `Please analyze this transcript and add speaker labels (Speaker 1, Speaker 2, etc.) to show when different people are talking:\n\n${transcriptionData.text}`
                }
              ],
              max_tokens: 1500
            })
          });
          
          if (!diarizationResponse.ok) {
            const errorData = await diarizationResponse.json();
            throw new Error(errorData.error?.message || 'Failed to diarize transcript');
          }
          
          const diarizationData = await diarizationResponse.json();
          const diarizedText = diarizationData.choices[0].message.content;
          
          // Display the diarized transcript
          transcriptionDiv.textContent = diarizedText;
          
          // Update state and UI
          appState = 'transcribed';
          updateUIState();
          showStatus('Transcription with speaker identification complete!');
          transcriptionSection.style.display = 'block';
          
        } catch (error) {
          console.error('Transcription error:', error);
          showError(error.message || 'Failed to transcribe audio');
          appState = 'recorded';
          updateUIState();
        }
      }
      
      async function summarizeTranscription() {
        if (!transcriptionDiv.textContent) {
          showError('No transcription available to summarize');
          return;
        }
        
        const apiKey = apiKeyInput.value.trim();
        if (!apiKey) {
          showError('Please enter your OpenAI API key');
          return;
        }
        
        const prompt = summaryPrompt.value.trim() || 'Summarize this conversation';
        
        try {
          // Update state and UI
          appState = 'summarizing';
          updateUIState();
          showStatus('Generating summary...');
          
          // Call OpenAI API for summarization
          const response = await fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${apiKey}`
            },
            body: JSON.stringify({
              model: "gpt-4o", // Use latest model
              messages: [
                {
                  role: "system",
                  content: "You are a helpful assistant that summarizes conversations accurately."
                },
                {
                  role: "user",
                  content: `${prompt}:\n\n${transcriptionDiv.textContent}`
                }
              ],
              max_tokens: 1000
            })
          });
          
          if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.error?.message || 'Failed to generate summary');
          }
          
          const summaryData = await response.json();
          summaryDiv.textContent = summaryData.choices[0].message.content;
          
          // Update state and UI
          appState = 'summarized';
          updateUIState();
          showStatus('Summary generated successfully!');
          summarySection.style.display = 'block';
          
        } catch (error) {
          console.error('Summarization error:', error);
          showError(error.message || 'Failed to generate summary');
          appState = 'transcribed';
          updateUIState();
        }
      }
      
      function abortProcess() {
        // Stop recording if in progress
        if (mediaRecorder && mediaRecorder.state === 'recording') {
          mediaRecorder.stop();
        }
        
        // Stop all tracks on the stream
        if (stream) {
          stream.getTracks().forEach(track => track.stop());
        }
        
        // Clear audio URL if it exists
        if (audioPreview.src) {
          URL.revokeObjectURL(audioPreview.src);
          audioPreview.src = '';
        }
        
        // Reset UI state
        appState = 'idle';
        updateUIState();
        showStatus('Process aborted.');
        
        // Clear displays
        transcriptionDiv.textContent = '';
        summaryDiv.textContent = '';
        audioPreviewSection.style.display = 'none';
        transcriptionSection.style.display = 'none';
        summarySection.style.display = 'none';
      }
      
      function downloadTranscript() {
        if (!transcriptionDiv.textContent) return;
        
        const filename = filenameInput.value.trim() || 'transcript';
        downloadText(transcriptionDiv.textContent, `${filename}.txt`);
      }
      
      function downloadSummary() {
        if (!summaryDiv.textContent) return;
        
        const filename = filenameInput.value.trim() || 'transcript';
        downloadText(summaryDiv.textContent, `Summary_${filename}.txt`);
      }
      
      function downloadText(text, filename) {
        const blob = new Blob([text], { type: 'text/plain' });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = filename;
        document.body.appendChild(a);
        a.click();
        document.body.removeChild(a);
        URL.revokeObjectURL(url);
      }
      
      function showStatus(message) {
        statusMessage.textContent = message;
        statusMessage.className = 'alert alert-info';
      }
      
      function showError(message) {
        statusMessage.className = 'alert alert-danger';
        
        // For 'Not supported' errors, provide a more helpful message
        if (message.includes('Not supported')) {
          statusMessage.innerHTML = `
            <strong>Recording option not supported</strong>
            <p>The selected recording source is not supported in this browser or environment. 
            This could be due to browser limitations or missing permissions.</p>
            <p>Recommendations:</p>
            <ul>
              <li>Try using the "Microphone Only" option which works in all browsers</li>
              <li>Make sure you grant all requested permissions when prompted</li>
              <li>If trying to record system audio, try a different browser (Chrome works best)</li>
            </ul>
          `;
        } else {
          statusMessage.textContent = message;
        }
        
        // Scroll to the error message
        statusMessage.scrollIntoView({ behavior: 'smooth', block: 'center' });
      }
      
      function updateUIState() {
        // Reset all buttons to default state
        startRecordingBtn.disabled = false;
        stopRecordingBtn.disabled = true;
        abortBtn.disabled = false;
        transcribeBtn.disabled = true;
        summarizeBtn.disabled = true;
        downloadTranscriptBtn.disabled = true;
        downloadSummaryBtn.disabled = true;
        
        // Enable/disable audio source selection
        microphoneOnlyRadio.disabled = false;
        systemAudioRadio.disabled = false;
        bothSourcesRadio.disabled = false;
        
        switch (appState) {
          case 'idle':
            // Only start recording is enabled
            break;
            
          case 'recording':
            // Can stop recording or abort
            startRecordingBtn.disabled = true;
            stopRecordingBtn.disabled = false;
            microphoneOnlyRadio.disabled = true;
            systemAudioRadio.disabled = true;
            bothSourcesRadio.disabled = true;
            break;
            
          case 'recorded':
            // Recording complete, can transcribe
            transcribeBtn.disabled = false;
            break;
            
          case 'transcribing':
          case 'diarizing':
            // Waiting for transcription, most buttons disabled
            startRecordingBtn.disabled = true;
            transcribeBtn.disabled = true;
            break;
            
          case 'transcribed':
            // Transcription complete, can summarize
            summarizeBtn.disabled = false;
            downloadTranscriptBtn.disabled = false;
            break;
            
          case 'summarizing':
            // Waiting for summary, most buttons disabled
            startRecordingBtn.disabled = true;
            summarizeBtn.disabled = true;
            break;
            
          case 'summarized':
            // Summary complete
            downloadSummaryBtn.disabled = false;
            break;
            
          default:
            break;
        }
        
        // Always enable/disable these based on content
        downloadTranscriptBtn.disabled = !transcriptionDiv.textContent;
        downloadSummaryBtn.disabled = !summaryDiv.textContent;
      }
    });
  </script>
</body>
</html>